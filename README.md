# ğŸ¯ Dynamic Pruning for Neural Network Lottery Tickets

Efficiently identify sparse sub-networksâ€”**winning tickets**â€”within a **single training cycle** by dynamically pruning weights during training.

[![Python](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.13-orange.svg)](https://pytorch.org/)
![NVIDIA GPU](https://img.shields.io/badge/GPU-NVIDIA%20L4-brightgreen.svg)

---

## ğŸ§  Biological Inspiration

<p align="center">
  <img src="./biological_pruning.gif" alt="Biological Pruning Animation" width="700px">
  <p><i>Synaptic pruning from childhood to adolescence inspires dynamic sparsity.</i></p>
</p>

The human brain undergoes synaptogenesis (creating many connections) followed by synaptic pruning (eliminating redundant ones) as it matures. Inspired by this, our dynamic pruning approach progressively removes less important neural connections during training, retaining only critical pathways.

---

## ğŸŒŸ Overview

The **Lottery Ticket Hypothesis (LTH)** posits that dense neural networks contain sparse subnetworks ("winning tickets") capable of matching the original network's performance when trained from scratch. However, conventional LTH methods face a significant challenge: they require computationally expensive cycles of:

1. Training a dense network to convergence
2. Pruning low-magnitude weights 
3. Rewinding remaining weights to their initial values
4. Retraining the sparse network from scratch
5. Repeating multiple times to find optimal tickets

Our approach revolutionizes this paradigm with a **dynamic pruning methodology** that operates within a single training cycle. Instead of the train-prune-retrain loop, we progressively mask weights during training based on their magnitude, allowing the network to naturally adapt to increasing sparsity levels.

This novel approach offers remarkable efficiency benefits:
- **Significant reduction in computational overhead**
- **Elimination of retraining cycles**
- **Competitive or superior accuracy** compared to traditional LTH methods
- **Biologically plausible** alignment with neural development

---

## ğŸš€ Highlights

- ğŸ”„ **Dynamic Sparsity:** Gradually masks low-magnitude weights during training (0% â†’ 80% sparsity).
- ğŸ“‰ **Single Training Cycle:** Integrates pruning directly into training, eliminating retraining.
- ğŸ“Š **High Accuracy at High Sparsity:** Maintains high accuracy (up to 94.91% on CIFAR-10, 99.51% on MNIST).
- ğŸ§  **Inspired by Neuroscience:** Mimics natural synaptic pruning processes in the brain.

---

## ğŸ“Š Results Summary

| Dataset   | Final Sparsity | Test Accuracy         | Epochs | Parameter Reduction |
|-----------|----------------|-----------------------|--------|---------------------|
| CIFAR-10  | 80%            | 91.98% â€“ 94.91%       | 200    | 11.2M â†’ 2.2M (5Ã—)   |
| MNIST     | 77%            | 99.45% â€“ 99.51%       | 30     | 11.2M â†’ 2.6M (4.3Ã—) |

<p align="center">
  <img src="./accuracy_loss_progression.gif" alt="Accuracy and Loss Progression" width="700px">
  <p><i>Train vs. Test accuracy and loss during training (MNIST).</i></p>
</p>

<p align="center">
  <img src="./dynamic_sparsity.gif" alt="Dynamic Sparsity Increase" width="600px">
  <p><i>Sparsity progression across epochs (MNIST).</i></p>
</p>

---

## ğŸ§ª Detailed Methodology

Our dynamic pruning method progressively identifies and removes less important neural connections during a single training cycle. Here's the clear workflow:

- **Initialization:**  
  Begin with fully active weights W and binary mask M = 1 (no pruning initially).

- **Dynamic Sparsity Schedule:**  
  Increase sparsity linearly from 0% (fully dense) to the target sparsity (e.g., 80%) across training epochs.  
  Compute sparsity level at epoch e:
  ```
  S_e = min(S_final, Î”S Ã— e)
  ```

- **Threshold-based Masking:**  
  Calculate pruning threshold Î¸ at the S_e-percentile of absolute weights |W|.  
  Update binary supermask M:
  ```
  M_ij = 0  if |W_ij| < Î¸
  M_ij = 1  otherwise
  ```

- **Training with Pruning:**  
  During forward and backward propagation, apply the mask:
  ```
  output = model(input; W âŠ™ M)
  ```
  where âŠ™ represents element-wise multiplication.

- **Iterative Update:**  
  Repeat the mask update and training for each epoch, allowing the network to dynamically adapt to the evolving sparsity.

<p align="center">
  <img src="./diagram-2.png" width="700">
  <p><i>Flowchart illustrating the dynamic pruning algorithm.</i></p>
</p>

---

## ğŸ“‚ Project Structure

```
ğŸ“ dynamic-lottery-tickets/
â”œâ”€â”€ biological_pruning.gif
â”œâ”€â”€ accuracy_loss_progression.gif
â”œâ”€â”€ dynamic_sparsity.gif
â”œâ”€â”€ diagram-2.png
â”œâ”€â”€ datasets.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ CIFAR10_With_Validation.ipynb
â”œâ”€â”€ CIFAR10_Without_ValidationSet.ipynb
â”œâ”€â”€ MNIST_with_Validation.ipynb
â”œâ”€â”€ MNIST_without_ValidationSet.ipynb
â”œâ”€â”€ Experiment_Cifar10_BatchSize_32.ipynb
â”œâ”€â”€ Experiment_Cifar10_LR_0.001.ipynb
â”œâ”€â”€ Experiment_Cifar10_LR_0.01.ipynb
â”œâ”€â”€ Experiment_MNIST_BS_32.ipynb
â”œâ”€â”€ Experiment_MNIST_LR_0.001.ipynb
â”œâ”€â”€ Experiment_MNIST_momentum0.5.ipynb
â”œâ”€â”€ Experiment_Cifar10_Epoch30_BS32.ipynb
â”œâ”€â”€ Experiment_Cifar10_Epoch400_BS128.ipynb
â”œâ”€â”€ Experiment_MNIST_with_Epoch100.ipynb
â””â”€â”€ Experiment_Cifar10_Resnet34.ipynb
```

---

## ğŸš€ Quickstart

```bash
git clone https://github.com/romanaq/dynamic-lottery-tickets.git
cd dynamic-lottery-tickets
pip install -r requirements.txt

# Train CIFAR-10
python main.py --dataset cifar10 --epochs 200 --batch-size 128 --final-sparsity 0.8

# Train MNIST
python main.py --dataset mnist --epochs 30 --batch-size 32 --final-sparsity 0.77
```

---

## ğŸ”­ Future Work
- Scaling to larger models (ResNet-50, Transformers).
- Advanced pruning criteria (gradient-based).
- Hardware integration for edge computing.
- Adaptive sparsity schedules.

---

## ğŸ‘¤ Author

**Romana Qureshi**  
*Master of Science in Artificial Intelligence, King Saud University*  
Researcher in Sparse Deep Learning

---

## ğŸ“¬ Contact

For questions or collaboration, contact me on [GitHub](https://github.com/romanaq).

---

## âš¡ Technologies
![Python](https://img.shields.io/badge/python-3.9-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.13-orange.svg)
![NVIDIA GPU](https://img.shields.io/badge/GPU-NVIDIA%20L4-brightgreen.svg)

---

<p align="center">
  <i>Inspired by nature. Built for efficiency.</i>
</p>
